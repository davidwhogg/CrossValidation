\documentclass[12pt]{article}

\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\train}{\mathrm{train}}
\newcommand{\valid}{\mathrm{valid}}
\newcommand{\dd}{\mathrm{d}}

\newcommand{\Dtr}{\ensuremath{D^{\rm TR}}}
\newcommand{\Dcv}{\ensuremath{D^{\rm CV}}}
\newcommand{\eqn}[1]{eq.~\ref{eq:#1}}

\begin{document}

\section{Outline}
\begin{itemize}
\item Bayesianism is a solid approach, but the Bayes integral is hard \&
  involves strong assumptions. {\bf Multi-dimensional integration
    is generally expensive}
\item CV is easy and has no -- or very few -- free parameters (we'll have to
  think about how choice of CV scheme affects this)
  {\bf Multi-dimensional optimization is generally cheaper}.
\item Both approaches have {\it predictive power}, and for this reason we
  should expect them to be related.
\item Main subject of the paper: how can we determine {\it a priori} when the
  two will give similar results?
\item (Aside on LTFDFCF \& model selection?  Or leave for a future paper...)
\end{itemize}

\section{Model Selection}
In model selection, we want to choose between a model $M_1$ which has
parameters $\theta_{M1}$, and a model $M_2$ which has parameters $\theta_{M2}$.
For example, imagine we would like to model the density of an observed
distribution of points in one dimension.  
$M_1$ might be a model in which our data is fit by a single
Gaussian: in this case the model parameters are $\theta_{M1} = \{\mu, \sigma\}$,
where $\mu$ is the mean of the distribution and $\sigma$ is the width of the
distribution.  $M_2$ might be a model in which our data is fit by two
Gaussians: in the model parameters are
$\theta_{M2} = \{\mu_1,\sigma_1,\mu_2,\sigma_2,w_{12}\}$ where $\mu_i,\sigma_i$
gives the mean and width of each Gaussian component, and $w_{12}$ gives the
relative weight of the two components. Notice that we have explicity
labeled the model parameters with the model name $M_1$ or $M_2$ as a reminder
that different models may have varying numbers of parameters.

Answering the question of which model best fits the data is a problem known
as {\it model selection}.  Two approaches are commonly used for this sort
of problem: the Bayesian approach of odds ratios and the frequentist approach
of cross-validation based on maximum likelihood
(should we mention AIC/BIC as well? Other cost functions?).
The frequentist approach is often the most simple: the cross-validation of
maximum likelihood is an optimization problem, which can be solved quickly
under most circumstances even in high-dimensional parameter spaces.
The Bayesian approach is in some sense the most rigorous: it allows for the
specific inclusion of prior information, and for the recovery of the full
posterior probability for each model.  This approach is not without its
weaknesses, however.  The Bayesian formalism is fundamentally a problem in
integration, and integration is much less efficient than optimization in
high dimensions.  Additionally, the Bayesian approach requires the
specification of a prior on the models: in cases where no actual prior
information exists, this specification becomes difficult, and even
so-called {\it uninformative priors} can have an effect on the final
result [ref?].

Nevertheless, both approaches are similar in that they can be shown to
have {\it predictive power}.  Because of this, we might expect them to
be related.  The goal of this work is to explicitly show the relationship
between the two methodologies, the assumptions used in relating the two,
and offer advice for deciding {\it a priori} when the faster frequentist
approach is a good approximation of the more rigorous but difficulty-laden
Bayesian approach.

\section{Frequentist Model Selection: Cross-Validation}
Cross-validation is a well-known approach in frequentism.  In the simplest
version of cross-validation, the data $D = \{d_i\}_{i=1}^n$
is divided into two samples:
the {\it training} data \Dtr used to train the model, and the
{\it cross-validation} data \Dcv which is used to validate the trained
model. It is important that these two samples are
non-overlapping and independent: in most cases this can be ensured by
allowing each observation to appear in just one of the sets, as in the
scheme aove.\footnote{cross-validation of correlated observations are
an interesting problem, but one we won't address in this paper [ref?].}.
To ensure independence, we will divide the data as follows: for $n$
observations we randomly choose $n_{tr}$ of the observations
as a training sample, and use the remainder as the cross-validation set.
Without loss of generality we can re-order the data so that the
the training data is given by $\Dtr = \{d_i\}_{i=1}^{n_{tr}}$,
and the cross-validation data is given by $\Dcv = \{d_i\}_{i=n_{tr}+1}^{n}$.

As mentioned above, cross-validation is a two-step process:
for each potential model $M$, the
training data \Dtr is used to construct the data likelihood as a function
of the vector of model parameters $\theta_M$.  This likelihood is of the form
\begin{equation}
  \label{eq:tr_likelihood}
  P(\Dtr|M, \theta_M, I) \propto \prod_{i=1}^{n_{tr}}P(d_i|M, \theta_M, I).
\end{equation}
The terms $P(d_i|M, \theta_M, I)$ specify the probability of obtaining a
particular observation $d_i$ given our model described by $M$ and $\theta_M$:
this probability can be thought of as a limiting frequency of results within
some small range around the particular values $d_i$: thus this is a
``frequentist'' approach.
Maximizing the likelihood in with respect to the model parameters $\theta_M$
in \eqn{tr_likelihood} gives the {\it maximum likelihood estimate} (MLE)
of the model parameters, which we call
\begin{equation}
  \hat{\theta}_M^{max} \leftarrow \arg\max_{\theta_M} P(\Dtr|M, \theta_M, I).
\end{equation}
Based on this MLE, we compute the cross-validation score, which is the
cross-validation likelihood for the MLE parameter estimate:
\begin{equation}
  L_{CV}(M) = P(\Dcv|M, \hat{\theta}_M^{max}, I)
\end{equation}
the model $M$ with the largest cross-validation likelihood $L_{CV}(M)$ is the
best-fit model chosen by the cross-validation procedure.

This approach can be very powerful, as well as computationally efficient.
However, by generalizing the notion of probability, we can approach this
from the Bayesian perspective, and gain further insight into the assumptions
made by the cross-validation approach.

\section{Bayesian Model Selection: the Odds Ratio}
The Bayesian approach is slightly different.  It seeks to compute the
{\it posterior model probability} $P(M|D,I)$, which by Bayes' theorem
can be expressed
\begin{equation}
  \label{eq:bayes_theorem}
  P(M|D,I) = \frac{P(D|M,I)P(M|I)}{P(D|I)}.
\end{equation}
The first term in the numerator is known as the {\it evidence},
and is expressible as an integral over the likelihood used
in cross-validation (see below).
The second term in the numerator is known
as the {\it prior}, and encodes any prior knowledge constraining the model.
The term in the denominator is a normalization, and is usually not
computed explicitly.

The expression in \eqn{bayes_theorem} follows directly from the basic axioms
of probability, but note that
we have subtley changed the meaning of our terms here:
in the previous section, the probability
$P(\cdot)$ could be thought of as the limiting frequency of observed events.
Here, we consider the probabilities of {\it models}, which in reality are
either true or false.  Our former approach to probability is not applicable!

This underlies the fundamental difference between Bayesianism and frequentism:
in the Bayesian approach, we expand the definition of probability to include
probabilistic statements about our own knowledge.  When we write an expression
of the probability of a model given some data, $P(M|D)$, it quantifies the
extent of our knowledge about the model, or in general the particular
parameters within the model.

Bayesian model selection involves the comparison between two models $M_1$
and $M_2$ given their posterior probabilities.  It is convenient to express
this comparison in terms of the {\it odds ratio}, given by
\begin{equation}
  O_{21} = \frac{P(M_2|D,I)}{P(M_1|D,I)}.
\end{equation}
Comparing to
The nice thing here is that the denominator (which is hard to compute) cancels
leaving
\begin{equation}
  O_{21} = \frac{P(D|M_1,I)}{P(D|M_2,I)}\frac{P(M_1|I)}{P(M_2|I)}
\end{equation}
The first quotient is known as the {\it Bayes Factor}, and the second is the
ratio of the model priors.

Computing the odds ratio involves computation of the evidence $P(D|M,I)$
for each model.  This evidence is difficult to compute because it expresses
our degree of knowledge of the truch of model $M$ -- not for any
particular choice of model parameters,
but integrated over {\it all} possible combinations of parameters allowed
by the model.  We can express this integral explicitly in terms of the
likelihood used in the frequentist approach:
\begin{equation}
  \label{eq:evidence_integral}
  P(D|M, I) = \int \dd^{N_M}\theta_M P(D|M, I, \theta_M)P(\theta_M|M, I),
\end{equation}
where $N_M$ is the number of parameters in model $M$.

This integral illustrates the two practical difficulties of the Bayesian
approach: first, the prior term $P(\theta_M|M,I)$ must be specified, and
even so-called ``uninformative'' priors can affect the result, especially
in the case of model comparison.  Second, the integral must be computed
over all $N_M$ dimensions.
As the number of dimensions grows, the computational cost of this step
can become prohibitively expensive.

\section{Comparing Bayesianism and Frequentism}
Comparing the two approaches, we see that the likelihood plays a central
role in both Bayesian and frequentist model selection.  The difference
is that in frequentism we {\it optimize} the likelihood, while in
Bayesianism we {\it integrate} the likelihood.  The relationship between
these approaches can be seen as follows.

Assume we're using single-fold cross-validation as described above, where
the data $D$ is split into a single training set \Dtr and a single
cross-valitation set \Dcv, with no overlap.
Because of the independence of \Dtr and \Dcv, we can write
\begin{eqnarray}
  P(D|I) &=& P(\Dtr,\Dcv|I)\\
             &=& P(\Dtr|I)P(\Dcv|I).
\end{eqnarray}
Using this identity, the evidence in \eqn{evidence_integral} can be expressed
\begin{equation}
  P(D|M,I) = \int \dd^N\theta_M P(\Dtr|M, I, \theta_M)
  P(\Dcv|M,I,\theta_M)P(\theta_M|M,I)
\end{equation}
The terms in this integral are straightforward to understand:
\begin{description}
  \item[$P(\Dtr|M, I, \theta_M)$] is the likelihood of the training data.
    In the cross-validation approach, the parameters $\hat{\theta}^{max}_M$ which
    maximize the likelihood are determined for each possible model $M$.
  \item[$P(\Dcv|M, I, \theta_M)$] is the likelihood of the cross-validation
    data.  In the cross-validation approach, this is maximized for $M$,
    for parameters $\theta_M = \hat{\theta}^{max}_M$ at their fixed, to choose
    the model $M$.
  \item[$P(\theta_M|M,I)$] is the prior on the model parameters.  This is
    implicitly assumed to be constant in the cross-validation approach.
\end{description}

In order to recover the cross-validation results within the Bayesian approach,
we can start by assuming the training likelihood is narrowly peaked around its
maximum likelihood value.  We can write this as
\begin{equation}
  P(\Dtr|M, I, \theta_M) \approx \delta(\theta_M - \hat{\theta}^{max}_M)
\end{equation}
Putting in this assumption yields
\begin{equation}
  P(D|M,I) \approx P(\Dcv|\hat{\theta}^{max}_M,M,I)P(\hat{\theta}^{max}_M|M,I)
\end{equation}
Returning to the Bayesian odds ratio, we find
\begin{equation}
  O_{21} \approx \frac{P(\Dcv|\hat{\theta}^{max}_{M1},M_1,I)}
                     {P(\Dcv|\hat{\theta}^{max}_{M2},M_2,I)}
                \frac{P(\hat{\theta}^{max}_{M1},M_1|I)}
                     {P(\hat{\theta}^{max}_{M2},M_2|I)}
\end{equation}
where we have used the identity $P(\theta|M,I)P(M|I) = P(M,\theta|I)$.
This looks very similar to the Bayesian odds ratio, except for a few
subtle changes:
\begin{itemize}
  \item We have replaced the full {\it integrated} data likelihood
    $P(D|M,I)$ with the likelihood at a particular value of the model
    parameters, $P(D|\hat{\theta}^{max}_{M},M,I)$.
  \item We have replaced the full {\it integrated} model prior $P(M|I)$
    with the prior for a particular choice of the model,
    $P(\hat{\theta}^{max}_{M},M|I)$.
\end{itemize}
If we assume uninformative priors such that the second quotient is unity,
then we recover the exact form of the cross-validation model.
This exercise has the benefit of explicitly showing which assumptions the
cross-validation approach makes as compared to the Bayesian approach.

First, in order to approximate the training likelihood as a delta function,
it must be the case that the training likelihood is very narrow compared to
both the model prior and the cross-validation likelihood.  In other words,
the model prior and cross-validation likelihood are slowly varying across
the significant portion of the training likelihood.  Though this is a strong
assumption, it is accurate (at least approximately) in many cases of interest:
an uninformative prior will be (by definition) much wider than the training
likelihood.  Additionally, because the cross-validation set is often much
smaller than the training set, it will give a much weaker constraint on the
model.  For this reason, it is safe to assume that the cross-validation
likelihood is wider than the training likelihood.

Second, the model priors must be roughly equal.  This allows us to drop the
second term in the above equation and recover the likelihood formulation.

We should summarize these assumptions here:
\begin{enumerate}
  \item The {\it training} likelihood does not have heavy tails, and is
    approximately symmetric and strongly peaked.
  \item The {\it cross-validation} likelihood is approximately constant
    across the important region of the training likelihood.
  \item The {\it parameter prior} is approximately constant across the important
    region of the training likelihood (that is, it is uninformative).
  \item The {\it model prior} for different models is approximately equal
    (that is, the model prior is uninformative).
\end{enumerate}
When these assumptions are met, the cross-validation and Bayesian approaches
will lead to similar results.

[think about some examples]

\section{Quantifying the Approximation}
In what situation can the frequentist approach be used as a substitute for
the Bayesian formulation?

\section{Examples}
We should come up with some good ones.

\section{Extension to Other CV Schemes}

\begin{eqnarray}
p(D\given\theta,H) &=& \prod_{n=1}^N p(D_n\given\theta,H)
\\
D &\equiv& \setof{D_n}_{n=1}^N
\\
D^\train_k &\equiv& \setof{D_n}_{n\in\train(k)}
\\
D^\valid_k &\equiv& \setof{D_n}_{n\in\valid(k)}
\\
p(D^\train_k\given\theta,H) &=& \prod_{n\in\train(k)} p(D_n\given\theta,H)
\\
p(D^\valid_k\given\theta,H) &=& \prod_{n\in\valid(k)} p(D_n\given\theta,H)
\\
p(\theta\given D^\train_k,H) &=& \frac{1}{Z_{Hk}}\,p(D^\train_k\given\theta,H)\,p(\theta\given H)
\\
Z_{Hk} &\equiv& \int p(D^\train_k\given\theta,H)\,p(\theta\given H)\,\dd\theta
\\
\theta_k &\leftarrow& \arg\max_\theta p(D^\train_k\given\theta,H)
\\
L_k &\equiv& p(D^\valid_k\given\theta_k,H)
\\
L &\equiv& \prod_{k=1}^K L_k
\\
\zeta_k &\equiv& \int p(D^\valid_k\given\theta,H)\,p(\theta\given D^\train_k,H)\,\dd\theta
\\
Z_H &\equiv& \int p(D\given\theta,H)\,p(\theta\given H)\,\dd\theta
\\
\zeta_k &=& \frac{Z_H}{Z_{Hk}}
\quad,
\end{eqnarray}

\end{document}
