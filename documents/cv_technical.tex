\documentclass[12pt]{article}

\usepackage{graphicx}
\graphicspath{{figs/}}

\usepackage{color}
\newcommand{\comment}[1]{{\color{red} [#1]}}
\newcommand{\highlight}[1]{{\color{green} #1}}

\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\train}{\mathrm{train}}
\newcommand{\valid}{\mathrm{valid}}
\newcommand{\dd}{\mathrm{d}}

\newcommand{\Dtr}{\ensuremath{D^{\rm TR}}}
\newcommand{\Dva}{\ensuremath{D^{\rm V}}}
\newcommand{\Ntr}{\ensuremath{N^{\rm TR}}}
\newcommand{\Nva}{\ensuremath{N^{\rm V}}}

\newcommand{\thetamax}[1]{\ensuremath{\hat{\theta}^{\max}_{#1}}}
\newcommand{\LCV}[1]{\ensuremath{L_{CV}}(#1)}
\newcommand{\LCVk}[1]{\ensuremath{L^{(k)}_{CV}}(#1)}
\newcommand{\LTPVk}[1]{\ensuremath{L^{(k)}_{TPV}}(#1)}

\newcommand{\Tkplus}{\ensuremath{T_k^{(+)}}}
\newcommand{\Tkminus}{\ensuremath{T_k^{(-)}}}

\newcommand{\eqn}[1]{eq.~(\ref{eq:#1})}
\newcommand{\eqns}[2]{eqs.~(\ref{eq:#1}) and (\ref{eq:#2})}
\newcommand{\fig}[1]{Fig.~\ref{fig:#1}}
\newcommand{\paper}{document}



\title{A Note on Model Comparison}
\author{Jacob T. VanderPlas and David W. Hogg}

\begin{document}
\maketitle

\begin{abstract}
There are some circumstances in which model comparison through cross-validation
provides an approximation to the fully marginalized likelihood or Bayesian
evidence.  We show that $K$-fold leave-one-out cross-validation is
approximately equivalent to Bayesian model testing when:
\begin{enumerate}
  \item The number of data points is very large, such that the constraint
    provided by any single point is inconsequential compared to the constraint
    provided by the entire data set, and
  \item An informative prior is used which is consistent with the data.
\end{enumerate}
\end{abstract}

\section{Introduction}
Cross-validation (CV) and the Bayes integral (i.e.~Bayesian evidence)
are both used very successfully in
the comparison of competing models.  The two approaches seem incomparable
because CV has no concept of prior information, while in Bayesianism the
prior is crucial.  Yet under certain assumptions the two techniques produce
quantitatively similar results.

This is important because while Bayesianism is the more probabilistically
rigorous approach, it can be computationally very difficult compared to
cross-validation.  In general, we assume that
\begin{enumerate}
  \item Maximum Likelihood is always easy to compute
  \item We can sample any posterior with MCMC or the like
  \item Though we can sample the prior, we cannot sample it finely enough
    to predictably ``hit'' the full-data likelihood.  That is, the Bayes 
    integral is hard to calculate without advanced techniques
    (nested sampling, etc.)
\end{enumerate}
These assumptions mean that cross-validation and other maximum
likelihood-based approaches are generally computationally straightforward,
while approaches based on Bayesianism are generally much
harder.\footnote{\comment{Add Sam's airplane CV quote here.}}

We will define our notation and outline these three approaches below, and
then show under which assumptions the three are related.

\subsection{Notation}
\begin{itemize}
  \item $I$ denotes any prior information which affects the problem.
  \item $M$ denotes a model which has a vector of model parameters $\theta_M$.
  \item $D$ is the observed data, consisting of $N$ independently drawn
    data points $d_n$;
    \begin{equation}
      D = \{d_n\}_{n=1}^N.
    \end{equation}
  \item  We will divide these data into $K$ overlapping training samples
    $T_k$ and disjoint validation samples $S_k$, which satisfy
    \begin{eqnarray}
      T_k \cup S_k = D\\
      T_k \cap S_k = \emptyset\\
      \forall_{k^\prime \ne k} S_{k^\prime} \cap S_k = \emptyset\\
      S_1 \cup S_2 \cup \cdots \cup S_K = D
    \end{eqnarray}
  \item We will further subdivide the training samples $T_k$ into subsets
    \Tkminus{} and \Tkplus{} such that
    \begin{eqnarray}
      \Tkminus = S_1 \cup S_2 \cup \cdots \cup S_{k-1}\\
      \Tkplus = S_{k+1} \cup S_{k+2} \cup \cdots \cup S_K\\
      T_k = \Tkminus \cup \Tkplus
    \end{eqnarray}
\end{itemize}

We will consider a cross-validation (CV) scheme based on the 
maximum likelihood estimate (MLE) for the training data,
\begin{equation}
  \thetamax{M,k} \equiv \arg\max_\theta \prod_{n \in T_k} P(d_n|\theta, M, I)
\end{equation}
with a single-fold CV score given by
\begin{equation}
  \label{eq:LCVk}
  \LCVk{M} \equiv P(S_k | \thetamax{M,k}, M, I).
\end{equation}
In the case of $K$-fold CV, the total CV score is
\begin{equation}
  \label{eq:LCV}
  \LCV{M} \equiv \prod_{k=1}^K \LCVk{M},
\end{equation}
The CV score is based on the assumption that data have {\it predictive power}:
that is, one portion of the data should constrain our knowledge of the
remainder.

In a Bayesian setting, model comparison proceeds via a fully marginalized
likelihood or {\it Bayes integral} or {\it Bayesian evidence}
\begin{equation}
  P(D|M,I) = \int P(D|\theta_M,M,I) P(\theta_M|M,I)\dd\theta_M
\end{equation}
Though the Bayes integral $P(D|M,I)$ is commonly thought of as an
integration of the parameter space, we can express it in a useful way
using probability identities.  We can chain together subsets of the
data to express the Bayes integral as
\begin{equation}
  \label{eq:bayes_chain}
  P(D|M,I) = \prod_{k=1}^K P(S_k|\Tkminus,M,I).
\end{equation}
This makes clear that the Bayesian approach is also based on a notion of
{\it predictive power} of the data, and is similar in spirit to
the CV approach.  This is also related to the ``sequential'' ideas
of Bayesian inference.

\section{Relationship Between the Approaches}
We seek a relationship between CV, which is computationally straightforward,
and the fully Bayesian approach, which is probabilistically rigorous, but
computationally challenging.  The approaches are similar in that they assume
the data have {\it predictive power} and evaluate the likelihood of each
individual datum using subsets of the remaining data.  Furthermore, the
results have the same dimensions, which makes it likely that they are
somehow related.

In this section we will use these insights to derive the explicit
relationship between the three approaches, and outline the assumptions
under which they give the same results.

We'll begin by defining an intermediate probabilistic quantity, the
Training-posterior Validation Likelihood (TPV)
\begin{equation}
  \label{eq:LTPVk}
  \LTPVk{M} \equiv P(S_k|T_k, M, I).
\end{equation}
This is the Bayesian ``answer'' to the CV score \LCVk{} given by
\eqn{LCVk}.  The single-fold CV \& TPV scores have a similar form,
and can be related by observing
\begin{eqnarray}
  \LTPVk{M} &=& P(S_k|T_k,M,I)\\
            &=& \int P(S_k|\theta_M,M,I) P(\theta_M|T_k,M,I)\dd\theta_M
\end{eqnarray}
where we have assumed independence of $T_k$ and $S_k$ when conditioned on
$\theta_M$, which is implied by the independence of the  $d_n$.
If we could make the approximation that
\begin{equation}
  P(\theta_M|T_k,M,I) \approx \delta(\theta_M - \thetamax{M,k})
\end{equation}
then the integral collapses and we are left with
\begin{equation}
  \LTPVk{M} \approx \LCVk{M}
\end{equation}
This delta function approximation is well-motivated only under the condition
that the model is much more tightly constrained by $T_k$ than by $S_k$.
That is, the likelihood $P(\theta_M|T_k,M,I)$ is much ``narrower'' in
$\theta_M$-space than the likelihood $P(S_k|\theta_M,M,I)$.
If this assumption is met, then the single-fold CV score \LCVk{M} is
approximately equivalent to the TPV score \LTPVk{M}.

The full $K$-fold CV score is a product of the $K$ \LCVk{M}s (\eqn{LCV}):
this motivates taking the product of the \LTPVk{M}s.
Although it is a probabilistic abomination, this product has the same units
as the $K$-fold CV score and the Bayes integral, and under some additional
assumptions approximates the Bayes integral, which we will show now.
Using the above definitions, we can show
\begin{eqnarray}
  \LCV{M} &\approx& \prod_{k=1}^K \LTPVk{M} \\
          &=& \prod_{k=1}^K P(S_k|\Tkminus,\Tkplus,M,I)\\
          &=& \prod_{k-1}^K \frac{P(\Tkplus|S_k,\Tkminus,M,I) P(S_k|\Tkminus,M,I)}
                               {P(\Tkplus|\Tkminus,M,I)}\\
          &=& P(D|M,I) \prod_{k=1}^K \frac{P(\Tkplus|S_k,\Tkminus,M,I)}
                                        {P(\Tkplus|\Tkminus,M,I)}
\end{eqnarray}
where the last line follows from \eqn{bayes_chain}.
If the quotient on the right hand side could ever be
aproximately unity, then we would have
\begin{equation}
  \LCV{M} \approx P(D|M,I).
\end{equation}
This quotient being unity is an interesting approximation: what it says is that
given a validation set \Tkminus{}, the addition of a new set $S_k$ of
observations contributes very little to the predictive power of the model.
This is most nearly approached in the extreme case of leave-one-out CV
where each $S_k$ contains a single datum $d_k$.

Even in this limit, while the approximation is likely valid for individual
terms of the product with $k \gg 1$
(where the model is already well constrained by the $k-1$ previous data),
for $k \sim 1$ the approximation is less likely to be accurate.
It will break down
{\it unless the prior is very informative compared to any single data point}.
For such an informative prior, the addition of a single data point will
have a negligible effect on the prediction, and allow the contribution 
of each point to the quotient to be close to 1.

\section{Discussion}
It is interesting that the presence of ``uninformative priors'' leads to a
large divergence between the results of cross-validation and Bayesian model
comparison.  But more curious still is that cross-validation {\it with no
prior input} comes close to the results of the Bayesian formalism which,
by assumption, includes a prior that is not only informative, but correct!
How can this be?

This apparently free information in CV comes from the fact that it
double-counts the data, and {\it we assume the data are consistent with the
model}.  That is, the data themselves are, in a sense, used to generate
the required informative prior.  If this assumption breaks down, the CV
score can still be useful, but it is no longer an approximation to the
Bayes integral.


\end{document}
