\documentclass[12pt]{article}

\usepackage{graphicx}
\graphicspath{{figs/}}

\usepackage{color}
\newcommand{\comment}[1]{{\color{red} [#1]}}
\newcommand{\highlight}[1]{{\color{green} #1}}

\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\train}{\mathrm{train}}
\newcommand{\valid}{\mathrm{valid}}
\newcommand{\dd}{\mathrm{d}}

\newcommand{\Dtr}{\ensuremath{D^{\rm TR}}}
\newcommand{\Dva}{\ensuremath{D^{\rm V}}}
\newcommand{\Ntr}{\ensuremath{N^{\rm TR}}}
\newcommand{\Nva}{\ensuremath{N^{\rm V}}}

\newcommand{\eqn}[1]{eq.~(\ref{eq:#1})}
\newcommand{\fig}[1]{Fig.~\ref{fig:#1}}
\newcommand{\paper}{document}



\title{A Note on Model Testing}
\author{Jacob T. VanderPlas and David W. Hogg}

\begin{document}
\maketitle

\section{Introduction}

This is a quick note about the relationship between several approaches to
model selection based on data.  We will explore cross-validation (CV),
\highlight{predictive probability criterion (PPC)}, and the Bayes integral
(i.e.~Bayesian evidence).  Our assumptions are as follows:
\begin{enumerate}
  \item Maximum Likelihood is easy to compute
  \item We can compute any posterior through sampling
  \item Though we can sample the prior, we cannot sample it finely enough
    to predictably ``hit'' the full-data likelihood.
\end{enumerate}
These assumptions mean that cross-validation and other maximum likelihood-based
approaches are generally computationally feasible, while approaches based
on Bayesianism are generally much harder.

We will define our notation and outline these three approaches below, and
then show under which assumptions the three are related.

\subsection{Notation}
\begin{itemize}
  \item $I$ is the prior information on $M$.
  \item $M$ is the model which has a vector of model parameters $\theta_M$.
    When we compare multiple models, these will be denoted $M_1$, $M_2$, ...
    with parameters $\theta_{M1}$, $\theta_{M2}$, ...
  \item $D$ is the observed data.  We will divide this data into multiple
    training samples $T_k$ and validation samples $S_k$, which satisfy
    $T_k \cup S_k = D$ and $T_k \cap S_k = \{\}$.
\end{itemize}


\subsection{Cross-validation (CV)}
Cross validation is a well-known technique for model comparison.  Using
maximum likelihood and the above notation, the training model is the
maximum likelihood estimate (MLE) for the training data,
\begin{equation}
  \hat{\theta}^{\max}_{M,k} \equiv \arg\max_\theta \prod_{n \in T_k} P(d_n|\theta, M, I)
\end{equation}
and the cross-validation score is
\begin{equation}
  L^{(k)}_{CV}(M) \equiv P(S_k | \hat{\theta}^{\max}_{M,k}, M, I).
\end{equation}
In the case of multiple ($K$-fold) cross-validation, the total
cross-validation score is
\begin{equation}
  L_{CV}(M) \equiv \prod_{k=1}^K L^{(k)}_{CV}(M)
\end{equation}
The model with the highest cross-validation score is said to be the
best model.

\subsection{\highlight{Predictive Probability Criterion (PPC)}}
The PPC is an attempt to cache the CV procedure in more formal, Bayesian
terms.  Each cross-validation score $L^{(k)}_{CV}(M)$ is really trying to
get at the quantity
\begin{equation}
  L^{(k)}_{PPC}(M) \equiv P(S_k|T_k, M, I).
\end{equation}
This can be thought of as a Bayesian approach to single cross-validation.

\subsection{Posterior Model Probability}
In a Bayesian setting, model selection is accomplished through computing
the posterior model probability
\begin{eqnarray}
  P(M|D,I) &\propto& P(D|M,I)P(M|I)\\
           &\propto& \int \dd\theta_M P(D|\theta_M,M,I)P(\theta_M|M,I) P(M|I)
\end{eqnarray}
This is the fully Bayesian approach, and requires computing a potentially
high-dimensional integral over the parameter space, as well as specifying
priors on the model $M$ and model parameters $\theta_M$.

The choice between two models is accomplished through the use of the odds
ratio,
\begin{eqnarray}
  O_{21} &\equiv& \frac{P(M_2|D,I)}{P(M_1|D,I)}\\
        &=& \frac{P(D|M_2,I)}{P(D|M_1,I)}\ \ \frac{P(M_2|I)}{P(M_1|I)}.
\end{eqnarray}
In cases where models are equally likely {\it a priori}, the final term
is often assumed to be unity.

As one final note, we can rewrite the Bayes integral $P(D|M,I)$ in a way
that will become useful later.  We will order the data with index $n$ and
define subsets $D_{n} = \{d_1, d_2 ... d_{n-1}\}$, such that $D = D_N$.
Then we can use probability axioms to write
\begin{equation}
  P(D|M,I) = \prod_{n=1}^N 
\end{equation}

\section{Relationship Between the Approaches}
We have briefly outlined three approaches to model selection based on model
selection.  They range from CV, which is a computationally straightforward
and statistically well-founded procedure, through the fully Bayesian approach
which is in some senses the most rigorous, but has philisophical as well
as computational difficulties.  Nevertheless, the three approaches are similar
in that they use the {\it predictive power} of data


\end{document}
