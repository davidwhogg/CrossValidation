\documentclass[12pt]{article}

\usepackage{graphicx}
\graphicspath{{figs/}}

\usepackage{color}
\newcommand{\comment}[1]{{\color{red} [#1]}}
\newcommand{\highlight}[1]{{\color{green} #1}}

\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\train}{\mathrm{train}}
\newcommand{\valid}{\mathrm{valid}}
\newcommand{\dd}{\mathrm{d}}

\newcommand{\Dtr}{\ensuremath{D^{\rm TR}}}
\newcommand{\Dva}{\ensuremath{D^{\rm V}}}
\newcommand{\Ntr}{\ensuremath{N^{\rm TR}}}
\newcommand{\Nva}{\ensuremath{N^{\rm V}}}

\newcommand{\thetamax}[1]{\ensuremath{\hat{\theta}^{\max}_{#1}}}
\newcommand{\LCV}[1]{\ensuremath{L_{CV}}(#1)}
\newcommand{\LCVk}[1]{\ensuremath{L^{(k)}_{CV}}(#1)}
\newcommand{\LPPC}[1]{\ensuremath{L_{PPC}}(#1)}
\newcommand{\LPPCk}[1]{\ensuremath{L^{(k)}_{PPC}}(#1)}

\newcommand{\Tkplus}{\ensuremath{T_k^{(+)}}}
\newcommand{\Tkminus}{\ensuremath{T_k^{(-)}}}

\newcommand{\eqn}[1]{eq.~(\ref{eq:#1})}
\newcommand{\eqns}[2]{eqs.~(\ref{eq:#1}) and (\ref{eq:#2})}
\newcommand{\fig}[1]{Fig.~\ref{fig:#1}}
\newcommand{\paper}{document}



\title{A Note on Model Testing}
\author{Jacob T. VanderPlas and David W. Hogg}

\begin{document}
\maketitle

\section{Introduction}

This is a quick note about the relationship between several approaches to
model selection based on data.  We will explore cross-validation (CV),
predictive probability criterion (PPC), and the Bayes integral
(i.e.~Bayesian evidence).  Our assumptions are as follows:
\begin{enumerate}
  \item Maximum Likelihood is easy to compute
  \item We can compute any posterior through sampling
  \item Though we can sample the prior, we cannot sample it finely enough
    to predictably ``hit'' the full-data likelihood.
\end{enumerate}
These assumptions mean that cross-validation and other maximum likelihood-based
approaches are generally computationally straightforward, while approaches based
on Bayesianism are generally much harder.

We will define our notation and outline these three approaches below, and
then show under which assumptions the three are related.

\subsection{Notation}
\begin{itemize}
  \item $I$ denotes any prior information which affects the problem.
  \item $M$ denotes a model which has a vector of model parameters $\theta_M$.
  \item $D$ is the observed data, generated by the model.
    We will divide this data into multiple training samples $T_k$
    and validation samples $S_k$, which satisfy
    $T_k \cup S_k = D$ and $T_k \cap S_k = \emptyset$.
\end{itemize}


\subsection{Cross-validation (CV)}
We will consider a cross-validation (CV) scheme based on the 
maximum likelihood estimate (MLE) for the training data,
\begin{equation}
  \thetamax{M,k} \equiv \arg\max_\theta \prod_{n \in T_k} P(d_n|\theta, M, I)
\end{equation}
with a single-fold CV score given by
\begin{equation}
  \label{eq:LCVk}
  \LCVk{M} \equiv P(S_k | \thetamax{M,k}, M, I).
\end{equation}
In the case of $K$-fold CV, the total CV score is
\begin{equation}
  \label{eq:LCV}
  \LCV{M} \equiv \prod_{k=1}^K \LCVk{M},
\end{equation}
which is maximized by the optimal model.  The CV score is based on the
assumption that data have {\it predictive power}: that is, one portion
of the data should constrain our knowledge of the remainder.


\subsection{Predictive Probability Criterion (PPC)}
In Bayesian terms, the spirit of CV is contained in the quantity
\begin{equation}
  \label{eq:LPPCk}
  \LPPCk{M} \equiv P(S_k|T_k, M, I),
\end{equation}
which we call the single-fold predictive probability criterion (PPC).
One might be tempted, in analogy with CV, to define a $K$-fold PPC,
\begin{equation}
  \label{eq:LPPC}
  \LPPC{M} \equiv \prod_{k=1}^K \LPPCk{M}.
\end{equation}
Unfortunately, this is not well-motivated within Bayesianism
as the individual probabilities in the product are not independent.


\subsection{Posterior Model Probability}
In a Bayesian setting, model selection is accomplished through
the posterior model probability
\begin{equation}
  P(M|D,I) = \frac{P(D|M,I)P(M|I)}{P(D|I)}
\end{equation}
The denominator is problematic, and can be removed by comparing two
models using the odds ratio,
\begin{eqnarray}
  O_{21} &\equiv& \frac{P(M_2|D,I)}{P(M_1|D,I)}\\
        &=& \frac{P(D|M_2,I)}{P(D|M_1,I)}\ \frac{P(M_2|I)}{P(M_1|I)}.
\end{eqnarray}
Often the final term is assumed to be unity, which is equivalent to
saying that the models are equally likely {\it a priori}.  In this case, the
odds ratio is given simply by the ratio of the global data likelihoods.

Though the Bayes integral $P(D|M,I)$ is most commonly evaluated through
integration of the parameter space, we can express it in a useful way
using probability identities.  We will order the data with index $n$ and
define subsets $D_{n} = \{d_1, d_2 \cdots d_{n-1}, d_n\}$, such that
$D_0 = \emptyset$ and $D_N = D$.
With this notation we can compactly write the Bayes integral
in terms of a product of individual probabilities for each of the data:
\begin{equation}
  \label{eq:bayes_chain}
  P(D|M,I) = \prod_{n=1}^N P(d_n|D_{n-1},M,I).
\end{equation}
This makes clear that the Bayesian approach is also based on a notion of
{\it predictive power} of the data, and is similar in spirit to
the CV and PPC approaches.

\section{Relationship Between the Approaches}
We have briefly outlined three approaches to model selection based on model
selection.  They range from CV, which is a computationally straightforward
and statistically well-founded procedure, through the fully Bayesian approach
which is in some senses the most rigorous, but is accompanied by
computational difficulties.  Nevertheless, the three approaches are similar
in that they assume the data has {\it predictive power}, and evaluate
the likelihood of each individual datum using subsets of the remaining data.

In this section we will use this insight to
derive the explicit relationship between the three
approaches, and outline the assumptions under which the three give the same
results.

\subsection{CV and PPC}
The single-fold CV \& PPC scores given by \eqns{LCVk}{LPPCk}
have a similar form, and can be related as follows:
\begin{eqnarray}
  \LPPCk{M} &=& P(S_k|T_k,M,I)\\
            &=& \frac{P(S_k,T_k|M,I)}{P(T_k|M,I)}\\
            &=& \frac{\int \dd\theta_M P(S_k,T_k,\theta_M|M,I)}
                     {\int \dd\theta_M P(T_k,\theta_M|M,I)}\\
            &=& \frac{\int \dd\theta_M P(S_k|T_k,\theta_M,M,I)
                                       P(\theta_M|T_k,M,I)P(T_k|M,I)}
                     {\int \dd\theta_M P(\theta_M|T_k,M,I) P(T_k|M,I)}\\
            &=& \frac{\int \dd\theta_M P(S_k|\theta_M,M,I)P(\theta_M|T_k,M,I)}
                     {\int \dd\theta_M P(\theta_M|T_k,M,I)}
\end{eqnarray}
where we have assumed that $S_k$ and $T_k$ are independent given $\theta_M$.
If we now make the assumption that
\begin{equation}
  P(\theta_M|T_k,M,I) \approx \delta(\theta_M - \thetamax{M,k})
\end{equation}
then the integrals collapse and we are left with
\begin{equation}
  \LPPCk{M} \approx \LCVk{M}
\end{equation}
This delta function approximation is well-motivated only under the condition
that the model is much more tightly constrained by $T_k$ than by $S_k$.

To reiterate, our two assumptions are that
\begin{enumerate}
  \item The training sample $T_k$ and validation sample $S_k$ are statistically
    independent given a  given a choice of model parameters $\theta_M$.
  \item The model $M$ is much more tightly constrained by the training sample
    $T_k$ than by the validation sample $S_k$.
\end{enumerate}
If these two assumptions are met, then the single-fold CV score \LCVk{M} is
approximately equivalent to the PPC score \LPPCk{M}.

The connection to cross-validation provides the
motivation for the definition of \LPPC{M} in \eqn{LPPC}.
If the above assumptions are met for all $k$, then we have
\begin{equation}
  \LPPC{M} \approx \LCVk{M}.
\end{equation}

\subsection{PPC and the Bayes Integral}
Here we will show that the $K$-fold PPC expression \LPPC{M} (\eqn{LPPC}),
though it is a strange expression in a Bayesian context, approximates the
Bayes integral $P(D|M,I)$ when certain assumptions are met.
To accomplish this we will define two more sets
\begin{eqnarray}
  \Tkplus &\equiv& \{d_n\}_{n=1}^{k-1}\\
  \Tkminus &\equiv& \{d_n\}_{n=k+1}^N.
\end{eqnarray}
Notice that with this definition, $D = d_k \cup \Tkplus \cup \Tkminus$
and $\Tkminus = D_{k-1}$ for $D_n$ defined above.
We'll assume a leave-one-out CV scheme, where $S_k =\{d_k\}$
and $T_k = \Tkminus \cup \Tkplus$, for $1 \le k \le K$.  With this we
have
\begin{eqnarray}
  \LCV{M} \approx \LPPC{M} &=& \prod_{k=1}^K P(d_k|\Tkminus,\Tkplus,M,I)\\
         &=& \prod_{k-1}^K \frac{P(\Tkplus|d_k,\Tkminus,M,I) P(d_k|\Tkminus,M,I)}
                               {P(\Tkplus|\Tkminus,M,I)}\\
         &=& P(D|M,I) \prod_{k=1}^K \frac{P(\Tkplus|d_k,\Tkminus,M,I)}
                                        {P(\Tkplus|\Tkminus,M,I)}
\end{eqnarray}
where the last line follows from \eqn{bayes_chain}.
If the quotient on the right hand side is aproximately unity, then we will
have
\begin{equation}
  \LCV{M} \approx \LPPC{M} \approx P(D|M,I).
\end{equation}
This quotient being unity is an interesting approximation: what it says is that
given a validation set \Tkminus{}, the addition of a new observation $d_k$
contributes very little to the predictive power of the model.  This is likely
a valid approximation for $k \gg 1$ where the model is already well constrained
by the $k-1$ previous data, but for $k \sim 1$ the approximation is less
likely to be correct.  It will break down
{\it unless the prior is very informative compared to a single data point}.
For such an informative prior, the addition of a single data point will
have a negligible effect on the prediction, and allow the contribution to the
quotient to be close to 1.

\subsection{Summary}
We have shown the assumptions under which three common model selection criteria
are approximately equivalent.  The single-fold CV score \LCVk{M}
is approximately equivalent to the predictive probability criterion
\LPPCk{M} if
\begin{enumerate}
  \item The training sample $T_k$ and validation sample $S_k$ are statistically
    independent given a choice of model parameters $\theta_M$, and
  \item The model is much more tightly constrained by $T_k$ than by $S_k$.
\end{enumerate}
The first condition is met for suitable data sets; the second condition is
commonly the case when the training sample is much larger than the validation
sample.

The $K$-fold predictive probability criterion \LPPC{M} is equivalent to
the Bayes integral $P(D|M,I)$ if
\begin{enumerate}
  \item[3] The constraint from a single point is inconsequential compared
    to the constraint from the model prior.
\end{enumerate}
If these three conditions are met, then $K$-fold leave-one-out cross-validation
will give approximately the same results as the Bayesian odds ratio.

\end{document}
