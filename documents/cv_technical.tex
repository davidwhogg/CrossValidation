\documentclass[12pt]{article}

\usepackage{graphicx}
\graphicspath{{figs/}}

\usepackage{color}
\newcommand{\comment}[1]{{\color{red} [#1]}}
\newcommand{\highlight}[1]{{\color{green} #1}}

\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\train}{\mathrm{train}}
\newcommand{\valid}{\mathrm{valid}}
\newcommand{\dd}{\mathrm{d}}

\newcommand{\Dtr}{\ensuremath{D^{\rm TR}}}
\newcommand{\Dva}{\ensuremath{D^{\rm V}}}
\newcommand{\Ntr}{\ensuremath{N^{\rm TR}}}
\newcommand{\Nva}{\ensuremath{N^{\rm V}}}

\newcommand{\thetamax}[1]{\ensuremath{\hat{\theta}^{\max}_{#1}}}
\newcommand{\LCV}[1]{\ensuremath{L_{CV}}(#1)}
\newcommand{\LCVk}[1]{\ensuremath{L^{(k)}_{CV}}(#1)}
\newcommand{\LTPVk}[1]{\ensuremath{L^{(k)}_{TPV}}(#1)}

\newcommand{\Tkplus}{\ensuremath{T_k^{(+)}}}
\newcommand{\Tkminus}{\ensuremath{T_k^{(-)}}}

\newcommand{\eqn}[1]{eq.~(\ref{eq:#1})}
\newcommand{\eqns}[2]{eqs.~(\ref{eq:#1}) and (\ref{eq:#2})}
\newcommand{\fig}[1]{Fig.~\ref{fig:#1}}
\newcommand{\paper}{document}



\title{A Note on Model Comparison}
\author{Jacob T. VanderPlas and David W. Hogg}

\begin{document}
\maketitle

\begin{abstract}
There are some circumstances in which model comparison through cross-validation
provides an approximation to the fully marginalized likelihood or Bayesian
evidence.  We show that $K$-fold leave-one-out cross-validation is
approximately equivalent to Bayesian model testing when:
\begin{enumerate}
  \item The number of data points is very large, such that the constraint
    provided by any single point is inconsequential compared to the constraint
    provided by the entire data set, and
  \item An informative prior is used which is consistent with the data.
\end{enumerate}
\end{abstract}

\section{Introduction}
Cross-validation (CV) and the Bayes integral (i.e.~Bayesian evidence)
are both used very successfully in
the comparison of competing models.  The two approaches seem incomparable
because CV has no concept of prior information, while in Bayesianism the
prior is crucial.  Yet under certain assumptions the two techniques produce
quantitatively similar results.

This is important because while Bayesianism is the more probabilistically
rigorous approach, it can be computationally very difficult compared to
cross-validation.  In general, we assume that
\begin{enumerate}
  \item Maximum Likelihood is always easy to compute
  \item We can sample any posterior with MCMC or the like
  \item Though we can sample the prior, we cannot sample it finely enough
    to predictably ``hit'' the full-data likelihood.  That is, the Bayes 
    integral is hard to calculate without advanced techniques
    (nested sampling, etc.)
\end{enumerate}
These assumptions mean that cross-validation and other maximum
likelihood-based approaches are generally computationally straightforward,
while approaches based on Bayesianism are generally much
harder.\footnote{\comment{Add Sam's airplane CV quote here.}}

We will define our notation and outline these three approaches below, and
then show under which assumptions the three are related.

\subsection{Notation}
\begin{itemize}
  \item $I$ denotes any prior information which affects the problem.
  \item $M$ denotes a model which has a vector of model parameters $\theta_M$.
  \item $D$ is the observed data, consisting of $N$ independently drawn
    data points $d_n$;
    \begin{equation}
      D = \{d_n\}_{n=1}^N.
    \end{equation}
  \item  We will divide these data into $K$ overlapping training samples
    $T_k$ and disjoint validation samples $S_k$, which satisfy
    \begin{eqnarray}
      T_k \cup S_k = D\\
      T_k \cap S_k = \emptyset\\
      \forall_{k^\prime \ne k} S_{k^\prime} \cap S_k = \emptyset\\
      S_1 \cup S_2 \cup \cdots \cup S_K = D
    \end{eqnarray}
  \item We will further subdivide the training samples $T_k$ into subsets
    \Tkminus{} and \Tkplus{} such that
    \begin{eqnarray}
      \Tkminus = S_1 \cup S_2 \cup \cdots \cup S_{k-1}\\
      \Tkplus = S_{k+1} \cup S_{k+2} \cup \cdots \cup S_K\\
      T_k = \Tkminus \cup \Tkplus
    \end{eqnarray}
\end{itemize}

We will consider a cross-validation (CV) scheme based on the 
maximum likelihood estimate (MLE) for the training data,
\begin{equation}
  \thetamax{M,k} \equiv \arg\max_\theta \prod_{n \in T_k} P(d_n|\theta, M, I)
\end{equation}
with a single-fold CV score given by
\begin{equation}
  \label{eq:LCVk}
  \LCVk{M} \equiv P(S_k | \thetamax{M,k}, M, I).
\end{equation}
In the case of $K$-fold CV, the total CV score is
\begin{equation}
  \label{eq:LCV}
  \LCV{M} \equiv \prod_{k=1}^K \LCVk{M},
\end{equation}
The CV score is based on the assumption that data have {\it predictive power}:
that is, one portion of the data should constrain our knowledge of the
remainder.

In a Bayesian setting, model comparison proceeds via a fully marginalized
likelihood or {\it Bayes integral} or {\it Bayesian evidence}
\begin{equation}
  P(D|M,I) = \int P(D|\theta_M,M,I) P(\theta_M|M,I)\dd\theta_M
\end{equation}
Though the Bayes integral $P(D|M,I)$ is commonly thought of as an
integration of the parameter space, we can express it in a useful way
using probability identities.  We can chain together subsets of the
data to express the Bayes integral as
\begin{equation}
  \label{eq:bayes_chain}
  P(D|M,I) = \prod_{k=1}^K P(S_k|\Tkminus,M,I).
\end{equation}
This makes clear that the Bayesian approach is also based on a notion of
{\it predictive power} of the data, and is similar in spirit to
the CV approach.  This is also related to the ``sequential'' ideas
of Bayesian inference, whereby the results of a previous analysis become
the prior for the subsequent analysis.

\section{Relationship Between the Approaches}
We seek a relationship between CV, which is computationally straightforward,
and the fully Bayesian approach, which is probabilistically rigorous, but
computationally challenging.  The approaches are similar in that they assume
the data have {\it predictive power} and evaluate the likelihood of each
individual datum using subsets of the remaining data.  Furthermore, the
results have the same dimensions, which makes it likely that they are
somehow related.

In this section we will use these insights to derive the explicit
relationship between the three approaches, and outline the assumptions
under which they give the same results.

We start with the Bayes integral, as expressed by \eqn{bayes_chain},
and express the probability in terms of an integral over $\theta_M$:
\begin{equation}
  P(D|M, I) = \prod_{k=1}^k
  \int P(S_k|\theta_M,M,I) 
  \left[P(\theta_M|T_k,M,I)\frac{P(S_k|\Tkminus,M,I)}
    {P(S_k|T_k,M,I)}
  \right]\dd\theta_M
\end{equation}
where we have assumed independence of $S_k$ and $T_k$ when conditioned
on $\theta_M$, which is implied by the independence of the  $d_n$.
Looking more closely at this, we see that if the term in brackets
could be approximated by $\delta(\theta_M - \thetamax{M,k})$, then
each of the $K$ integrals would collapse and we'd recover exactly \eqn{LCV}.
Thus, with this single approximation, we have $P(D|M, I) \approx \LCV{M}$
and the two model selection schemes agree.

This approximation can be viewed in two distinct parts:
\begin{enumerate}
  \item It requires $P(\theta_M|T_k,M,I)$ to be much ``narrower''
    in $\theta_M$-space than $P(S_k|\theta_M, M, I)$.
    If this is the case, then integrating over the
    distribution is approximately equivalent to evaluating
    $P(S_k|\theta_m,M,I)$ at
    the maximum value of the distribution, \thetamax{M,k}.
  \item It requires the quotient in the brackets to be approximately unity
    for each $k$.
\end{enumerate}
The first of these approximations is likely to be true when the size of the
validation sample $S_k$ is much smaller than the size of the training sample
$T_k$, and when the data $D$ strongly constrain the model parameters
$\theta_M$.

The second piece is less likely to be an accurate approximation.
If the size of each $S_k$ is small and the samples are statistically similar,
then this approximation is likely to be accurate for $k \gg 1$:
the addition of a small amount of new training data will not greatly
affect the posterior for the validation sample.
For $k \sim 1$, however, this approximation is more stringent:
it says that the
posterior probability of the validation sample {\it given the prior alone}
is equal to the posterior probability {\it given the full training set}.
Furthermore, because there is nothing special about the ordering of $k$,
this more stringent constraint should be approximately true for
each of the $K$ validation sets!

This observation leads to the insight regarding what defines cross-validation.
The only way for the second approximation to be true is if the prior
information $I$ contains the training data $T_k$.  If this is the case, then
the quotient reduces trivially to unity, and the cross-validation score
approximates the Bayes integral.

\end{document}
