\documentclass[12pt]{article}

\usepackage{graphicx}
\graphicspath{{figs/}}

\usepackage{color}
\newcommand{\comment}[1]{{\color{red} [#1]}}
\newcommand{\highlight}[1]{{\color{green} #1}}

\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\train}{\mathrm{train}}
\newcommand{\valid}{\mathrm{valid}}
\newcommand{\dd}{\mathrm{d}}

\newcommand{\Dtr}{\ensuremath{D^{\rm TR}}}
\newcommand{\Dva}{\ensuremath{D^{\rm V}}}
\newcommand{\Ntr}{\ensuremath{N^{\rm TR}}}
\newcommand{\Nva}{\ensuremath{N^{\rm V}}}

\newcommand{\thetamax}[1]{\ensuremath{\hat{\theta}^{\max}_{#1}}}
\newcommand{\LCV}[1]{\ensuremath{L_{CV}}(#1)}
\newcommand{\LCVk}[1]{\ensuremath{L^{(k)}_{CV}}(#1)}
\newcommand{\LTPVk}[1]{\ensuremath{L^{(k)}_{TPV}}(#1)}

\newcommand{\Tkplus}{\ensuremath{T_k^{(+)}}}
\newcommand{\Tkminus}{\ensuremath{T_k^{(-)}}}

\newcommand{\eqn}[1]{eq.~(\ref{eq:#1})}
\newcommand{\eqns}[2]{eqs.~(\ref{eq:#1}) and (\ref{eq:#2})}
\newcommand{\fig}[1]{Fig.~\ref{fig:#1}}
\newcommand{\paper}{document}



\title{A Note on Model Comparison}
\author{Jacob T. VanderPlas and David W. Hogg}

\begin{document}
\maketitle

\begin{abstract}
There are some circumstances in which model comparison through cross-validation
provides an approximation to the fully marginalized likelihood or Bayesian
evidence.  We show that $K$-fold leave-one-out cross-validation is
approximately equivalent to Bayesian model testing when:
\begin{enumerate}
  \item The number of data points is very large, such that the constraint
    provided by any single point is inconsequential compared to the constraint
    provided by the entire data set, and
  \item An informative prior is used which is consistent with the data.
\end{enumerate}
\end{abstract}

\section{Introduction}
Cross-validation (CV) and the Bayes integral (i.e.~Bayesian evidence)
are both used very successfully in
the comparison of competing models.  The two approaches seem incomparable
because CV has no concept of prior information, while in Bayesianism the
prior is crucial.  Yet under certain assumptions the two techniques produce
quantitatively similar results.

This is important because while Bayesianism is the more probabilistically
rigorous approach, it can be computationally very difficult compared to
cross-validation.  In general, we assume that
\begin{enumerate}
  \item Maximum Likelihood is always easy to compute
  \item We can sample any posterior with MCMC or the like
  \item Though we can sample the prior, we cannot sample it finely enough
    to predictably ``hit'' the full-data likelihood.  That is, the Bayes 
    integral is hard to calculate without advanced techniques
    (nested sampling, etc.)
\end{enumerate}
These assumptions mean that cross-validation and other maximum
likelihood-based approaches are generally computationally straightforward,
while approaches based on Bayesianism are generally much
harder.\footnote{\comment{Add Sam's airplane CV quote here.}}

We will define our notation and outline these three approaches below, and
then show under which assumptions the three are related.

\subsection{Notation}
\begin{itemize}
  \item $I$ denotes any prior information which affects the problem.
  \item $M$ denotes a model which has a vector of model parameters $\theta_M$.
  \item $D$ is the observed data, consisting of $N$ independently drawn
    data points $d_n$;
    \begin{equation}
      D = \{d_n\}_{n=1}^N.
    \end{equation}
  \item  We will divide these data into $K$ overlapping training samples
    $T_k$ and disjoint validation samples $S_k$, which satisfy
    \begin{eqnarray}
      T_k \cup S_k = D\\
      T_k \cap S_k = \emptyset\\
      \forall_{k^\prime \ne k} S_{k^\prime} \cap S_k = \emptyset\\
      S_1 \cup S_2 \cup \cdots \cup S_K = D
    \end{eqnarray}
  \item We will further subdivide the training samples $T_k$ into subsets
    \Tkminus{} and \Tkplus{} such that
    \begin{eqnarray}
      \Tkminus = S_1 \cup S_2 \cup \cdots \cup S_{k-1}\\
      \Tkplus = S_{k+1} \cup S_{k+2} \cup \cdots \cup S_K\\
      T_k = \Tkminus \cup \Tkplus
    \end{eqnarray}
\end{itemize}

We will consider a cross-validation (CV) scheme based on the 
maximum likelihood estimate (MLE) for the training data,
\begin{equation}
  \thetamax{M,k} \equiv \arg\max_\theta \prod_{n \in T_k} P(d_n|\theta, M, I)
\end{equation}
with a single-fold CV score given by
\begin{equation}
  \label{eq:LCVk}
  \LCVk{M} \equiv P(S_k | \thetamax{M,k}, M, I).
\end{equation}
In the case of $K$-fold CV, the total CV score is
\begin{equation}
  \label{eq:LCV}
  \LCV{M} \equiv \prod_{k=1}^K \LCVk{M},
\end{equation}
The CV score is based on the assumption that data have {\it predictive power}:
that is, one portion of the data should constrain our knowledge of the
remainder.

In a Bayesian setting, model comparison proceeds via a fully marginalized
likelihood or {\it Bayes integral} or {\it Bayesian evidence}
\begin{equation}
  P(D|M,I) = \int P(D|\theta_M,M,I) P(\theta_M|M,I)\dd\theta_M
\end{equation}
Though the Bayes integral $P(D|M,I)$ is commonly thought of as an
integration of the parameter space, we can express it in a useful way
using probability identities.  We can chain together subsets of the
data to express the Bayes integral as
\begin{equation}
  \label{eq:bayes_chain}
  P(D|M,I) = \prod_{k=1}^K P(S_k|\Tkminus,M,I).
\end{equation}
This makes clear that the Bayesian approach is also based on a notion of
{\it predictive power} of the data, and is similar in spirit to
the CV approach.  This is also related to the ``sequential'' ideas
of Bayesian inference, whereby the results of a previous analysis become
the prior for the subsequent analysis.

\section{Relationship Between the Approaches}
We seek a relationship between CV, which is computationally straightforward,
and the fully Bayesian approach, which is probabilistically rigorous, but
computationally challenging.  The approaches are similar in that they assume
the data have {\it predictive power} and evaluate the likelihood of each
individual datum using subsets of the remaining data.  Furthermore, the
results have the same dimensions, which makes it likely that they are
somehow related.

In this section we will use these insights to derive the explicit
relationship between the three approaches, and outline the assumptions
under which they give the same results.

We start with the Bayes integral, as expressed by \eqn{bayes_chain},
and express the probability in terms of an integral over $\theta_M$:
\begin{equation}
  \label{eq:full_approximation}
  P(D|M, I) = \prod_{k=1}^k
  \left[\frac{P(S_k|\Tkminus,M,I)}
    {P(S_k|T_k,M,I)}
  \right]
  \int P(S_k|\theta_M,M,I) P(\theta_M|T_k,M,I) \dd\theta_M
\end{equation}
where we have assumed independence of $S_k$ and $T_k$ when conditioned
on $\theta_M$, which is implied by the independence of the  $d_n$.
Looking more closely at this, we see that if $P(\theta_M|T_k,M,I)$
could be approximated by $\delta(\theta_M - \thetamax{M,k})$,
then each of the $K$ integrals would collapse and we'd recover an
equation proportional to \eqn{LCV}, up to the quotient in brackets.
If the quotient in brackets were further approximately unity, then
\eqn{LCV} would be recovered exactly.  Thus, under these two
approximations, we have $P(D|M,I) \approx \LCV{M}$ and the two
model selection schemes agree.

These approximations will never be met exactly, but in certain situations
may be accurate enough to be practical:
\begin{enumerate}
  \item The first approximation requires
    $P(\theta_M|T_k,M,I)$ to be much ``narrower''
    in $\theta_M$-space than $P(S_k|\theta_M, M, I)$.
    If this is the case, then integrating over the
    distribution is approximately equivalent to evaluating
    $P(S_k|\theta_m,M,I)$ at
    the maximum value of the distribution, \thetamax{M,k}.
  \item The second approximation requires the quotient in the brackets
    to be approximately unity for each $k$: in other words the likelihood
    of the validation set $S_k$ must be nearly equal given either
    the partial training set \Tkminus{} or the much larger training set
    $T_k$.
\end{enumerate}
The first of these approximations is likely to be sufficiently accurate
when the size of the validation sample $S_k$
is much smaller than the size of the training sample
$T_k$, and when the data $D$ strongly constrain the model parameters
$\theta_M$.

The second piece is less likely to be an accurate approximation.
If the size of each $S_k$ is small and the samples are statistically similar,
then this approximation is likely to be accurate for $k \gg 1$:
the addition of a small amount of new training data will not greatly
affect the posterior for the validation sample.
For $k \sim 1$, however, this approximation is more stringent:
it says that the
posterior probability of the validation sample {\it given the prior alone}
is equal to the posterior probability {\it given the full training set}.
Furthermore, because there is nothing special about the ordering of $k$,
this more stringent constraint should be approximately true for
each of the $K$ validation sets!

This observation leads to the insight regarding what defines cross-validation.
The only way for the second approximation to be true is if the prior
information $I$ contains the training data $T_k$.  If this is the case, then
the quotient reduces trivially to unity, and the cross-validation score
approximates the Bayes integral.

\section{Example}

We will explore this by considering a simple example, of two single-parameter
models:
\begin{eqnarray}
  {\rm \bf Model\ 1:} & y(t) = a_1t\\
  {\rm \bf Model\ 2:} & y(t) = a_2\sin(t)
\end{eqnarray}
We consider the models on the domain $-2 \le t \le 2$.  \fig{model_description}
shows these two models, along with $N=64$ points drawn from each model with
gaussian error of width $\sigma_y = 1$.  The model parameters are drawn from
a $\mathcal{N}(0, 1)$ prior distribution: with a particular choice of random
seed, this gives $a_1 = a_2 = 1.79$.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{plot_toy_example_1.pdf}
  \caption{The two models used for the simple example.  64 points are drawn
    from each model with gaussian noise of width $\sigma_y = 1$.
    The best-fit lines of each model are shown for each data.}
  \label{fig:model_description}
\end{figure}

The first of the two approximations says that
$P(\theta_M|T_k,M,I)$ is much narrower than
$P(S_k|\theta_M,M,I)$ in $\theta_M$-space.
We expect this to be the case when $S_k$ is much smaller than $T_k$.
\fig{delta_approx} compares these two curves for the points shown in
the upper panel of \fig{model_description}, using a single point in the
validation set $S_1$.  For large datasets and leave-one-out cross-validation,
this approximation will be valid.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{plot_toy_example_2.pdf}
  \caption{}
  \label{fig:delta_approx}
\end{figure}

The second approximation says that the quotient in \eqn{full_approximation}
must be close to unity.  This expression can be rewritten as
\begin{equation}
  \label{eq:quotient_approx_2}
  \prod_{k=1}^K \frac{P(S_k|\Tkminus,M,I)}
    {P(S_k|T_k,M,I)}
  = P(D|M,I)\prod_{k=1}^K \frac{P(T_k|M,I)}{P(D|M,I)},
\end{equation}
which involves only expressions of the Bayes integral for various subsets of
the data.  We plot the components of this expression in
\fig{quotient_approx}.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{plot_toy_example_3.pdf}
  \caption{}
  \label{fig:quotient_approx}
\end{figure}

It is interesting to see that, although the quotient is not near unity in either
case, the {\it deviations} from unity are very similar.  If this is the case,
then we can expect CV and Bayesian model {\it comparison} to give the same
results.

{\it This figure shows that the contributions of each point to the deviation
  between CV and Bayes are similar for each model... I've been trying to
  figure out how we can estimate the deviation given in \eqn{quotient_approx_2}
  without actually calculating the Bayes integrals in the expression...
  haven't figured it out yet.}

For model 1 data, the results are as follows:\\
\begin{tabular}{llll}
\hline
& Model 1 & Model 2 & $\Delta_{12}$\\
\hline
$\log(L_{CV})$ & -52.37 &  -57.18  & -4.82\\
$\log P(D|M,I)$ & -53.86 &  -58.34  & -4.48\\
\hline
\end{tabular}\\
In this case, the cross-validation and Bayes method approximately
agree, and find that the linear model is a better fit to the linear data.
The difference between the two measures for each model can be accounted
for in the expressions derived above.

\end{document}
